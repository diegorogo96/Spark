{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5662eab1\r\n",
       "import spark.implicits._\r\n",
       "defined class Bloggers\r\n",
       "bloggers: String = LearningSparkV2-master/databricks-datasets/learning-spark-v2/blogs.json\r\n",
       "bloggersDS: org.apache.spark.sql.Dataset[Bloggers] = [Campaigns: array<string>, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"Empleados\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "// In Scala\n",
    "case class Bloggers(id:BigInt, first:String, last:String, url:String, published:String,\n",
    "hits: BigInt, campaigns:Array[String])\n",
    "\n",
    "\n",
    "val bloggers=\"LearningSparkV2-master/databricks-datasets/learning-spark-v2/blogs.json\"\n",
    "\n",
    "val bloggersDS = spark\n",
    " .read\n",
    " .format(\"json\")\n",
    " .option(\"path\", bloggers)\n",
    " .load()\n",
    " .as[Bloggers]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "|           Campaigns|    First| Hits| Id|   Last|Published|              Url|\n",
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "| [twitter, LinkedIn]|    Jules| 4535|  1|  Damji| 1/4/2016|https://tinyurl.1|\n",
      "| [twitter, LinkedIn]|   Brooke| 8908|  2|  Wenig| 5/5/2018|https://tinyurl.2|\n",
      "|[web, twitter, FB...|    Denny| 7659|  3|    Lee| 6/7/2019|https://tinyurl.3|\n",
      "|       [twitter, FB]|Tathagata|10568|  4|    Das|5/12/2018|https://tinyurl.4|\n",
      "|[web, twitter, FB...|    Matei|40578|  5|Zaharia|5/14/2014|https://tinyurl.5|\n",
      "| [twitter, LinkedIn]|  Reynold|25568|  6|    Xin| 3/2/2015|https://tinyurl.6|\n",
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "bloggersDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|     uname|usage|\n",
      "+---+----------+-----+\n",
      "|  0|user-Gpi2C|  525|\n",
      "|  1|user-DgXDi|  502|\n",
      "|  2|user-M66yO|  170|\n",
      "|  3|user-xTOn6|  913|\n",
      "|  4|user-3xGSz|  246|\n",
      "|  5|user-2aWRN|  727|\n",
      "|  6|user-EzZY1|   65|\n",
      "|  7|user-ZlZMZ|  935|\n",
      "|  8|user-VjxeG|  756|\n",
      "|  9|user-iqf1P|    3|\n",
      "+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7ea89a27\r\n",
       "import spark.implicits._\r\n",
       "import scala.util.Random._\r\n",
       "defined class Usage\r\n",
       "r: scala.util.Random = scala.util.Random@33dfc15\r\n",
       "data: scala.collection.immutable.IndexedSeq[Usage] = Vector(Usage(0,user-Gpi2C,525), Usage(1,user-DgXDi,502), Usage(2,user-M66yO,170), Usage(3,user-xTOn6,913), Usage(4,user-3xGSz,246), Usage(5,user-2aWRN,727), Usage(6,user-EzZY1,65), Usage(7,user-ZlZMZ,935), Usage(8,user-VjxeG,756), Usage(9,user-iqf1P,3), Usage(10,user-91S1q,794), Usage(11,user-qHNj0,501), Usage(12,user-7hb94,460), Usage(13,user-bz0WF,142), Usage(14,user-71nwy,479), Usage(15,user-7GZz1,823), Usage(16,user-1CSk6,140), Usage(17,user-WPzlL,246), Usage(18,user-VaEit,451), Us...\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "// In Scala\n",
    "import scala.util.Random._\n",
    "// Our case class for the Dataset\n",
    "case class Usage (uid:Int, uname:String, usage: Int) extends Serializable\n",
    "val r = new scala.util.Random(42)\n",
    "// Create 1000 instances of scala Usage class \n",
    "// This generates data on the fly\n",
    "val data = for (i <- 0 to 1000)\n",
    " yield (Usage(i, \"user-\" + r.alphanumeric.take(5).mkString(\"\"),\n",
    " r.nextInt(1000)))\n",
    "// Create a Dataset of Usage typed data\n",
    "val dsUsage = spark.createDataset(data)\n",
    "dsUsage.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 8) (EM2021002738.bosonit.local executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @11d6de79; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @24d6589a)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 8) (EM2021002738.bosonit.local executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @11d6de79; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @24d6589a)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\r",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\r",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r",
      "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r",
      "  at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\r",
      "  at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:827)\r",
      "  ... 45 elided\r",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @11d6de79; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @24d6589a)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r",
      "  at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31)\r",
      "  at org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\r",
      "  at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "dsUsage\n",
    " .filter(d => d.usage > 900)\n",
    " .orderBy(desc(\"usage\"))\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class UsageCost\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "// Create a new case class with an additional field, cost\n",
    "case class UsageCost(uid: Int, uname:String, usage: Int, cost: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computeUserCostUsage: (u: Usage)UsageCost\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute the usage cost with Usage as a parameter\n",
    "// Return a new object, UsageCost\n",
    "def computeUserCostUsage(u: Usage): UsageCost = {\n",
    " val v = if (u.usage > 750) u.usage * 0.15 else u.usage * 0.50\n",
    " UsageCost(u.uid, u.uname, u.usage, v)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 12) (EM2021002738.bosonit.local executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @11d6de79; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @24d6589a)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 12) (EM2021002738.bosonit.local executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @11d6de79; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @24d6589a)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)\r",
      "  ... 45 elided\r",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @11d6de79; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @24d6589a)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "// Use map() on our original Dataset\n",
    "dsUsage.map(u => {computeUserCostUsage(u)}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
