{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@36492084\r\n",
       "csvFile: String = LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n",
       "df: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession \n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    ".master(\"yarn\")\n",
    ".config(\"hive.merge.mapfiles\", \"false\")\n",
    ".config(\"hive.merge.tezfiles\", \"false\")\n",
    ".config(\"parquet.enable.summary-metadata\", \"false\")\n",
    ".config(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    ".config(\"hive.merge.smallfiles.avgsize\", \"160000000\")\n",
    " .enableHiveSupport()\n",
    ".config(\"hive.exec.dynamic.partition\", \"true\")\n",
    ".config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    ".config(\"spark.sql.orc.impl\", \"native\")\n",
    ".config(\"spark.sql.orc.parquet.binaryAsString\", \"true\")\n",
    ".config(\"spark.sql.orc.parquet.writeLegacyFormat\", \"true\")\n",
    " .getOrCreate()\n",
    "\n",
    "// Path to data set \n",
    "val csvFile=\"LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csvFile)\n",
    "\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where((col(\"delay\") > 120) && (col(\"origin\") === \"SFO\") && (col(\"destination\") === \"ORD\"))\n",
    "    .orderBy(desc(\"delay\"))\n",
    "    .show(10, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db3\")\n",
    "spark.sql(\"USE learn_spark_db3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl2 (date STRING, delay INT, distance INT, origin STRING, destination STRING) USING csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "4: error: ';' expected but ',' found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:4: error: ';' expected but ',' found.\r",
      "        SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\r",
      "                   ^\r",
      "<console>:5: error: unclosed character literal (or use \" for string literal \"SFO\")\r",
      "        origin = 'SFO';\r",
      "                     ^\r",
      "<console>:9: error: ';' expected but ',' found.\r",
      "        SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\r",
      "                   ^\r",
      "<console>:10: error: unclosed character literal (or use \" for string literal \"JFK\")\r",
      "        origin = 'JFK';\r",
      "                     ^\r",
      ""
     ]
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO';\n",
    "\n",
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: unclosed string literal\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: unclosed string literal\r",
      "       spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\r",
      "                 ^\r",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING json\n",
    "    OPTIONS (\n",
    "      path 'LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    " distance INT, origin STRING, destination STRING) \n",
    " USING csv OPTIONS (PATH \n",
    " 'LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table] = [name: string, database: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csv_file: String = LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csv_file = \"LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = date STRING, delay INT, distance INT, origin STRING, destination STRING\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "33: error: overloaded method value csv with alternatives:\r",
     "output_type": "error",
     "traceback": [
      "<console>:33: error: overloaded method value csv with alternatives:\r",
      "  (paths: String*)org.apache.spark.sql.DataFrame <and>\r",
      "  (csvDataset: org.apache.spark.sql.Dataset[String])org.apache.spark.sql.DataFrame <and>\r",
      "  (path: String)org.apache.spark.sql.DataFrame\r",
      " cannot be applied to (String, schema: String)\r",
      "       val flights_df = spark.read.csv(csv_file, schema=schema)\r",
      "                                   ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val flights_df = spark.read.csv(csv_file, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Table `managed_us_delay_flights_tbl` already exists.\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Table `managed_us_delay_flights_tbl` already exists.\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:626)\r",
      "  ... 38 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1011250|   55|   SFO|        JFK|\n",
      "|1012230|    0|   SFO|        JFK|\n",
      "|1010705|   -7|   SFO|        JFK|\n",
      "|1010620|   -3|   SFO|        MIA|\n",
      "|1010915|   -3|   SFO|        LAX|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\"\"\").show(5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1010900|   14|   JFK|        LAX|\n",
      "|1011200|   -3|   JFK|        LAX|\n",
      "|1011900|    2|   JFK|        LAX|\n",
      "|1011700|   11|   JFK|        LAS|\n",
      "|1010800|   -1|   JFK|        SFO|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM us_origin_airport_JFK_tmp_view\"\"\").show(5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+--------------------+\n",
      "|           name|     description|         locationUri|\n",
      "+---------------+----------------+--------------------+\n",
      "|        default|default database|file:/C:/Users/di...|\n",
      "| learn_spark_db|                |file:/C:/Users/di...|\n",
      "|learn_spark_db2|                |file:/C:/Users/di...|\n",
      "|learn_spark_db3|                |file:/C:/Users/di...|\n",
      "+---------------+----------------+--------------------+\n",
      "\n",
      "+--------------------+---------------+-----------+---------+-----------+\n",
      "|                name|       database|description|tableType|isTemporary|\n",
      "+--------------------+---------------+-----------+---------+-----------+\n",
      "|managed_us_delay_...|learn_spark_db3|       null|  MANAGED|      false|\n",
      "|managed_us_delay_...|learn_spark_db3|       null|  MANAGED|      false|\n",
      "|us_delay_flights_tbl|           null|       null|TEMPORARY|       true|\n",
      "+--------------------+---------------+-----------+---------+-----------+\n",
      "\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "|       name|description|dataType|nullable|isPartition|isBucket|\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "|       date|       null|     int|    true|      false|   false|\n",
      "|      delay|       null|     int|    true|      false|   false|\n",
      "|   distance|       null|     int|    true|      false|   false|\n",
      "|     origin|       null|  string|    true|      false|   false|\n",
      "|destination|       null|  string|    true|      false|   false|\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listDatabases().show(5, true)\n",
    "spark.catalog.listTables().show(5, true)\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\").show(5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usFlightsDF: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n",
       "usFlightsDF2: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "val usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/diego.rodriguez/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val file= \"C:/Users/diego.rodriguez/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\n",
    "val df = spark.read.format(\"parquet\").load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    " USING parquet\n",
    " OPTIONS (\n",
    " path \"C:/Users/diego.rodriguez/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/diego.rodriguez/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/diego.rodriguez/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "val df = spark.read.format(\"json\").load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|   CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = ./LearningSparkV2-master/chapter3/data/prueba/\r\n",
       "df: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"./LearningSparkV2-master/chapter3/data/prueba/\"\n",
    "val df = spark.read.format(\"parquet\").load(file)\n",
    "\n",
    "df.show(5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+--------------------+--------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "|ALSUnit|             Address|       AvailableDtTm|Battalion| Box|  CallDate|CallFinalDisposition|CallNumber|        CallType|CallTypeGroup|City|    Delay|FinalPriority|FirePreventionDistrict|IncidentNumber|            Location|        Neighborhood|NumAlarms|OriginalPriority|Priority|        RowID|StationArea|SupervisorDistrict|UnitID|UnitSequenceInCallDispatch|UnitType| WatchDate|Zipcode|\n",
      "+-------+--------------------+--------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+--------------------+--------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "|  false|2000 Block of CAL...|01/11/2002 01:51:...|      B04|3362|01/11/2002|               Other|  20110016|  Structure Fire|         null|  SF|     2.95|            3|                     4|       2003235|(37.7895840679362...|     Pacific Heights|        1|               3|       3|020110016-T13|         38|                 5|   T13|                         2|   TRUCK|01/10/2002|  94109|\n",
      "|   true|0 Block of SILVER...|01/11/2002 03:01:...|      B10|6495|01/11/2002|               Other|  20110022|Medical Incident|         null|  SF|      4.7|            3|                    10|       2003241|(37.7337623673897...|Bayview Hunters P...|        1|               3|       3|020110022-M17|         42|                10|   M17|                         1|   MEDIC|01/10/2002|  94124|\n",
      "|   true|MARKET ST/MCALLIS...|01/11/2002 02:39:...|      B03|1455|01/11/2002|               Other|  20110023|Medical Incident|         null|  SF|2.4333334|            3|                     3|       2003242|(37.7811772186856...|          Tenderloin|        1|               3|       3|020110023-M41|         01|                 6|   M41|                         2|   MEDIC|01/10/2002|  94102|\n",
      "|  false|APPLETON AV/MISSI...|01/11/2002 04:16:...|      B06|5626|01/11/2002|               Other|  20110032|    Vehicle Fire|         null|  SF|      1.5|            3|                     6|       2003250|(37.7388432849018...|      Bernal Heights|        1|               3|       3|020110032-E11|         32|                 9|   E11|                         1|  ENGINE|01/10/2002|  94110|\n",
      "|  false|1400 Block of SUT...|01/11/2002 06:01:...|      B04|3223|01/11/2002|               Other|  20110043|          Alarms|         null|  SF|3.4833333|            3|                     4|       2003259|(37.7872890372638...|    Western Addition|        1|               3|       3|020110043-B04|         03|                 2|   B04|                         2|   CHIEF|01/10/2002|  94109|\n",
      "+-------+--------------------+--------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+--------------------+--------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = ./LearningSparkV2-master/chapter3/data/pruebajson/\r\n",
       "df: org.apache.spark.sql.DataFrame = [ALSUnit: boolean, Address: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"./LearningSparkV2-master/chapter3/data/pruebajson/\"\n",
    "val df = spark.read.format(\"json\").load(file)\n",
    "\n",
    "df.show(5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------+--------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|       _c0|   _c1|           _c2|             _c3|       _c4|       _c5|                 _c6|                 _c7|                 _c8| _c9|   _c10|     _c11|       _c12|_c13|            _c14|    _c15|         _c16|   _c17|         _c18|     _c19|    _c20|                _c21|                _c22|              _c23|                _c24|                _c25|         _c26|     _c27|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------+--------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCal...|FirePreventionDis...|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                   2|                   4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                   1|                  10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                   2|                   3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                   1|                   6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------+--------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = ./LearningSparkV2-master/chapter3/data/pruebacsv/\r\n",
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"./LearningSparkV2-master/chapter3/data/pruebacsv/\"\n",
    "val df = spark.read.format(\"csv\").load(file)\n",
    "\n",
    "df.show(5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
