{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002738.bosonit.local:4042\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1620373435921)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cubed: Long => Long = $Lambda$2011/0x0000000801436428@560eb4ba\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "// Create cubed function\n",
    "val cubed = (s: Long) => {\n",
    " s * s * s\n",
    "}\n",
    "// Register UDF\n",
    "spark.udf.register(\"cubed\", cubed)\n",
    "// Create temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@610e30ca\r\n",
       "emple: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 4 more fields]\r\n",
       "res2: Array[org.apache.spark.sql.Row] = Array([10001,1953-09-02,Georgi,Facello,M,1986-06-26], [10002,1964-06-02,Bezalel,Simmel,F,1985-11-21], [10003,1959-12-03,Parto,Bamford,M,1986-08-28], [10004,1954-05-01,Chirstian,Koblick,M,1986-12-01], [10005,1955-01-21,Kyoichi,Maliniak,M,1989-09-12])\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"Empleados\")\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "val emple = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"employees\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"diego\")\n",
    " .load()\n",
    "\n",
    "emple.createOrReplaceTempView(\"emple_tbl\")\n",
    "\n",
    "emple.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dept: org.apache.spark.sql.DataFrame = [dept_no: string, dept_name: string]\r\n",
       "res3: Array[org.apache.spark.sql.Row] = Array([d009,Customer Service], [d005,Development], [d002,Finance], [d003,Human Resources], [d001,Marketing])\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dept = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"departments\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"diego\")\n",
    " .load()\n",
    "\n",
    "dept.createOrReplaceTempView(\"dept_tbl\")\n",
    "\n",
    "dept.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salario: org.apache.spark.sql.DataFrame = [emp_no: int, salary: int ... 2 more fields]\r\n",
       "res4: Array[org.apache.spark.sql.Row] = Array([10001,60117,1986-06-26,1987-06-26], [10001,62102,1987-06-26,1988-06-25], [10001,66074,1988-06-25,1989-06-25], [10001,66596,1989-06-25,1990-06-25], [10001,66961,1990-06-25,1991-06-25])\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salario = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"salaries\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"diego\")\n",
    " .load()\n",
    "\n",
    "salario.createOrReplaceTempView(\"salario_tbl\")\n",
    "\n",
    "salario.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titulo: org.apache.spark.sql.DataFrame = [emp_no: int, title: string ... 2 more fields]\r\n",
       "res5: Array[org.apache.spark.sql.Row] = Array([10001,Senior Engineer,1986-06-26,9999-01-01], [10002,Staff,1996-08-03,9999-01-01], [10003,Senior Engineer,1995-12-03,9999-01-01], [10004,Engineer,1986-12-01,1995-12-01], [10004,Senior Engineer,1995-12-01,9999-01-01])\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val titulo = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"titles\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"diego\")\n",
    " .load()\n",
    "\n",
    "titulo.createOrReplaceTempView(\"titulo_tbl\")\n",
    "\n",
    "titulo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dept_emp: org.apache.spark.sql.DataFrame = [emp_no: int, dept_no: string ... 2 more fields]\r\n",
       "res6: Array[org.apache.spark.sql.Row] = Array([10001,d005,1986-06-26,9999-01-01], [10002,d007,1996-08-03,9999-01-01], [10003,d004,1995-12-03,9999-01-01], [10004,d004,1986-12-01,9999-01-01], [10005,d003,1989-09-12,9999-01-01])\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val dept_emp = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"dept_emp\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"diego\")\n",
    " .load()\n",
    "\n",
    "dept_emp.createOrReplaceTempView(\"dept_emp_tbl\")\n",
    "\n",
    "dept_emp.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+----------------+------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|           title|salary|\n",
      "+------+----------+----------+---------+------+----------+----------------+------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 40000|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 43519|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 46265|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 46865|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 47837|\n",
      "+------+----------+----------+---------+------+----------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT e.emp_no, e.birth_date, e.first_name, e.last_name, e.gender,\n",
    "e.hire_date, t.title, s.salary FROM emple_tbl as e\n",
    "INNER JOIN salario_tbl as s on e.emp_no=s.emp_no\n",
    "INNER JOIN titulo_tbl as t on e.emp_no=t.emp_no\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1: Array[Int] = Array(35, 36, 32, 30, 40, 42, 38)\r\n",
       "t2: Array[Int] = Array(31, 32, 34, 55, 56)\r\n",
       "tC: org.apache.spark.sql.DataFrame = [celsius: array<int>]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1 = Array(35, 36, 32, 30, 40, 42, 38)\n",
    "val t2 = Array(31, 32, 34, 55, 56)\n",
    "val tC = Seq(t1, t2).toDF(\"celsius\")\n",
    "tC.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "tC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// In Scala/Python\n",
    "// Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \n",
    "FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// In Scala/Python\n",
    "// Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " filter(celsius, t -> t > 38) as high \n",
    " FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// In Scala/Python\n",
    "// Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " exists(celsius, t -> t = 38) as threshold\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             celsius|avgFahrenheit|\n",
      "+--------------------+-------------+\n",
      "|[35, 36, 32, 30, ...|           96|\n",
      "|[31, 32, 34, 55, 56]|          105|\n",
      "+--------------------+-------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// In Scala/Python\n",
    "// Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " aggregate(\n",
    " celsius, \n",
    " 0, \n",
    " (t, acc) -> t + acc, \n",
    " acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    " ) as avgFahrenheit \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "delaysPath: String = LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n",
       "airportsPath: String = LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\r\n",
       "airports: org.apache.spark.sql.DataFrame = [City: string, State: string ... 2 more fields]\r\n",
       "delays: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n",
       "foo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Set file paths\n",
    "val delaysPath =\n",
    " \"LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "val airportsPath =\n",
    " \"LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    "\n",
    "// Obtain airports data set\n",
    "val airports = spark.read\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferschema\", \"true\")\n",
    " .option(\"delimiter\", \"\\t\")\n",
    " .csv(airportsPath)\n",
    "airports.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "// Obtain departure Delays data set\n",
    "val delays = spark.read\n",
    " .option(\"header\",\"true\")\n",
    " .csv(delaysPath)\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    "delays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "// Create temporary small table\n",
    "val foo = delays.filter(\n",
    " expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND \n",
    " date like '01010%' AND delay > 0\"\"\"))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bar: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Scala\n",
    "// Union two tables\n",
    "val bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// In Scala\n",
    "foo.join(\n",
    " airports.as('air),\n",
    " $\"air.IATA\" === $\"origin\"\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\r\n",
       "foo2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "val foo2 = foo.withColumn(\n",
    " \"status\",\n",
    " expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo3: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "val foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo4: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "val foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    " FROM departureDelays\n",
    "WHERE origin = 'SEA'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    " FROM departureDelays WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    " CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    " FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show(10, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
